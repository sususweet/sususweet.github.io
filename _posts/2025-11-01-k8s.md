---
layout: post
title: CentOS 搭建 K8S
categories: [Cloud]
description: CentOS 搭建 K8S
keywords: Centos, K8S
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---

## 版本信息

- linux发行版：CentOS 8.2
  
- 安装docker-ce 20.10.5（所有机器）
  
- 设置k8s环境准备条件（所有机器）
  
- 安装k8s v1.23.0 master管理节点
  
- 安装k8s v1.23.0 node工作节点
  
- 安装flannel（master）
  

## 安装docker-ce（所有机器）

所有安装k8s的机器都需要安装docker：

```shell
# 安装yum源管理工具
yum install -y yum-utils device-mapper-persistent-data lvm2

# 配置阿里云的docker源
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

# 安装docker-ce
yum install -y docker-ce

# 启动docker
systemctl enable docker && systemctl start docker
```

## 设置k8s环境准备条件（所有机器）

安装k8s的机器需要2个CPU和2g内存以上的配置，执行以下脚本做一些准备操作。所有安装k8s的机器都需要这一步操作。

你可以不关闭防火墙，只需要开启这些端口就行了  
MASTER节点  
6443* Kubernetes API server  
2379-2380 etcd server client API  
10250 Kubelet API  
10251 kube-scheduler  
10252 kube-controller-manager  
10255 Read-only Kubelet API (Heapster)

Worker节点

10250 Kubelet API  
10255 Read-only Kubelet API (Heapster)  
30000-32767 Default port range for NodePort Services. Typically, these ports would need to be exposed to external load-balancers, or other external consumers of the application itself.

```shell
# 关闭防火墙
systemctl disable firewalld
systemctl stop firewalld

# 关闭selinux
# 临时禁用selinux
setenforce 0
# 永久关闭 修改/etc/sysconfig/selinux文件设置
sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux
sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux
# 临时禁用交换分区
swapoff -a
# 永久禁用交换分区，打开/etc/fstab注释掉swap那一行。
sed -i 's/.*swap.*/#&/' /etc/fstab

# 修改内核参数（可选）
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

# 修改主机名
hostnamectl set-hostname k8s-master
```

## 安装k8s master管理节点

以上两个步骤检查完毕之后，继续以下步骤。由于官方k8s源在google，国内无法访问，这里使用阿里云yum源。

### 安装kubeadm、kubelet、kubectl

```shell
# 执行配置k8s阿里云源
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 安装kubeadm、kubectl、kubelet
yum install -y kubectl-1.23.0 kubeadm-1.23.0 kubelet-1.23.0 

# 启动kubelet服务
systemctl enable kubelet && systemctl start kubelet
```

### 初始化k8s

以下开始安装k8s需要用到的docker镜像，因为无法访问到国外网站，所以这条命令使用的是国内的阿里云的源(`registry.aliyuncs.com/google_containers`)。另一个非常重要的是：这里的`--apiserver-advertise-address`使用的是master和node间能互相ping通的ip，可以是内网ip，也可以是外网ip。

内网ip配置命令：

```shell
kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.23.0 --apiserver-advertise-address 192.168.99.104 --pod-network-cidr=10.244.0.0/16 --token-ttl 0
```

公网ip配置命令：

```shell
# xxx.xx.xxx.xx为master机器的公网ip地址
kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.23.0 --apiserver-advertise-address xxx.xx.xxx.xx  --pod-network-cidr=10.244.0.0/16
```

还可以用配置文件的形式来初始化集群：

```shell
   cat > /root/kubeadm-config.yaml << EOF
   apiVersion: kubeadm.k8s.io/v1beta2
   kind: ClusterConfiguration
   kubernetesVersion: v1.20.6
   controlPlaneEndpoint: 192.168.121.84:6443
   imageRepository: registry.aliyuncs.com/google_containers
   apiServer:
     certSANs:
     - 192.168.121.100 # vip地址
   networking:
     podSubnet: 10.244.0.0/16  # pod 容器网段
     serviceSubnet: 10.10.0.0/16  # service 网段范围  这三个地址的网段不可重复、重叠
   ---
   apiVersion: kubeproxy.config.k8s.io/v1alpha1
   kind: KubeProxyConfiguration
   mode: ipvs
   EOF
```

执行初始化

```
kubeadm init --config kubeadm-config.yaml  --upload-certs
```

如果中途出现报错，修复之后需要先执行`kubeadm reset`，再重新执行。

这条命令执行时会卡在`[preflight] You can also perform this action in beforehand using ''kubeadm config images pull`，大概需要2分钟，请耐心等待。上面安装完后，k8s会提示你输入如下命令，复制粘贴过来，执行即可。

```shell
mkdir -p HOME/.kube 
sudo cp -i /etc/kubernetes/admin.conf HOME/.kube/config  
sudo chown (id -u):(id -g) $HOME/.kube/config
```

### 记住node加入集群的命令

上面`kubeadm init`执行成功后会返回node节点加入集群的命令，等会要在node节点上执行，需要保存下来，如果忘记了，可以使用如下命令获取。

```shell
kubeadm token create --print-join-command
```

至此，安装master节点完毕。可以使用`kubectl get nodes`查看一下，此时master处于NotReady状态。

## 安装k8s node工作节点

如果还没安装docker，请参照本文步骤二安装docker-ce（所有机器）安装。如果没设置k8s环境准备条件，请参照本文步骤三设置k8s环境准备条件（所有机器）执行。

以上两个步骤检查完毕之后，继续以下步骤。

### 安装kubeadm、kubelet

```shell
# 执行配置k8s阿里云源
cat < /etc/yum.repos.d/kubernetes.repo  
[kubernetes]  
name=Kubernetes  
baseurl=[kubernetes-yum-repos-kubernetes-el7-x86_64安装包下载_开源镜像站-阿里云](https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/)  
enabled=1  
gpgcheck=1  
repo_gpgcheck=1  
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg  
EOF

# 安装kubeadm、kubectl、kubelet
yum install -y kubeadm-1.23.0 kubelet-1.23.0

# 启动kubelet服务
systemctl enable kubelet && systemctl start kubelet
```

### 加入集群

这里加入集群的命令每个人都不一样，可以登录master节点，使用`kubeadm token create --print-join-command`来获取。获取后执行如下。

```shell
kubeadm join 192.168.99.104:6443 --token ncfrid.7ap0xiseuf97gikl \  
--discovery-token-ca-cert-hash sha256:47783e9851a1a517647f1986225f104e81dbfd8fb256ae55ef6d68ce9334c6a2
```

加入成功后，可以在master节点上使用`kubectl get nodes`命令查看到加入的节点。

如果想要node节点也能够执行`kubectl`命令，需要将master的`/etc/kubernetes/admin.conf`移到node节点上，同时配置环境变量，同master。

## 安装Calico或flannel（master机器）

以上步骤安装完后，机器搭建起来了，但状态还是NotReady，master机器需要安装网络插件。node节点上也要有对应的镜像，保证网络通畅。

Calico安装：

```shell
kubectl apply -f https://calico-v3-25.netlify.app/archive/v3.25/manifests/calico.yaml
```

可参考：
https://github.com/projectcalico/calico/releases?page=3
https://www.cnblogs.com/khtt/p/16563088.html

flannel安装：

```shell
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl apply -f kube-flannel.yml
```
## Portainer 安装
```shell
docker run -d --name portainer -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v /app/portainer_data:/data --restart always --privileged=true portainer/portainer-ce:latest
```
## 搭建多Master集群
### 扩容Master节点

把 k8s-master01 节点的证书拷贝到 k8s-master02 上

```shell
ssh k8s-master02 "cd /root && mkdir -p /etc/kubernetes/pki/etcd &&mkdir -p ~/.kube/"
scp /etc/kubernetes/pki/ca.crt k8s-master02:/etc/kubernetes/pki/
scp /etc/kubernetes/pki/ca.key k8s-master02:/etc/kubernetes/pki/
scp /etc/kubernetes/pki/sa.key k8s-master02:/etc/kubernetes/pki/
scp /etc/kubernetes/pki/sa.pub k8s-master02:/etc/kubernetes/pki/
scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-master02:/etc/kubernetes/pki/
scp /etc/kubernetes/pki/front-proxy-ca.key k8s-master02:/etc/kubernetes/pki/
scp /etc/kubernetes/pki/etcd/ca.crt k8s-master02:/etc/kubernetes/pki/etcd/
scp /etc/kubernetes/pki/etcd/ca.key k8s-master02:/etc/kubernetes/pki/etcd/
```

在 master01 上查看加入节点的命令

```shell
kubeadm token create --print-join-command
```

得到类似如下的结果：

```shell
kubeadm join 192.168.2.124:6443 --token 0nlw6z.oxjj0igkmyowqxcu     --discovery-token-ca-cert-hash sha256:c28890784e52f5720669d2d750c81b9c3ba603d8fce07d76b8213a0599a98a31 
```

在上述命令后拼接`--control-plane`，使用拼接后的命令在 master02 节点上开始扩容 master01：

```shell
kubeadm join 192.168.2.124:6443 --token 0nlw6z.oxjj0igkmyowqxcu     --discovery-token-ca-cert-hash sha256:c28890784e52f5720669d2d750c81b9c3ba603d8fce07d76b8213a0599a98a31 --control-plane
```

第一次加入集群的时候会有以下报错

```
[preflight] Running pre-flight checks
	[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.6. Latest validated version: 19.03
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
error execution phase preflight: 
One or more conditions for hosting a new control plane instance is not satisfied.

unable to add a new control plane instance a cluster that doesn't have a stable controlPlaneEndpoint address

Please ensure that:
* The cluster has a stable controlPlaneEndpoint address.
* The certificates that must be shared among control plane instances are provided.


To see the stack trace of this error execute with --v=5 or higher
```

解决办法如下：查看master01上的kubeadm-config.yaml

```shell
kubectl -n kube-system get cm kubeadm-config -o yaml
```

发现没有`controlPlaneEndpoint`，所以要添加`controlPlaneEndpoint`

```shell
kubectl -n kube-system edit cm kubeadm-config
```

大概在这么个位置：
```yaml
kind: ClusterConfiguration
kubernetesVersion: v1.18.0
controlPlaneEndpoint: 192.168.2.124:6443//添加这个
```
出现如下信息则表示扩容成功
```
To start administering your cluster from this node, you need to run the following as a regular user:

    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
```

在新扩容的master02上配置 kubectl 的配置文件 config，相当于对 kubectl 进行授权，这样 kubectl 命令可以使用这个证书对 k8s 集群进行管理

```shell
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

可以查看一下集群状态
```
[root@k8s-master01 ~]# kubectl get nodes
NAME           STATUS   ROLES                  AGE     VERSION
k8s-master01   Ready    control-plane,master   7h53m   v1.20.6
k8s-master02   Ready    control-plane,master   4m7s    v1.20.6
k8s-node01     Ready    <none>                 7h14m   v1.20.6
k8s-node02     Ready    <none>                 7h14m   v1.20.6
```
以此类推，可以扩容master03节点，进而搭建多Master节点高可用k8s集群。

### 搭建四层负载均衡

在服务器上关闭防火墙和 selinux，并安装nginx

```shell
systemctl stop firewalld.service 
setenforce 0
yum install nginx -y
```

接下来在配置文件设置 Nginx 的四层负载均衡，指定 k8s 集群 2 台 master 的节点 ip 和 6443 端口
```json
vi /etc/nginx/nginx.conf
events {
    worker_connections  1024;
}
# 添加这个
stream {
    # 日志格式
    log_format  main  '$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent';
        # 日志存放路径
        access_log  /var/log/nginx/k8s-access.log  main;
    # master 调度资源池
    upstream k8s-apiserver {
        server 192.168.80.10:6443;
        server 192.168.80.20:6443;
    }
    server {
        listen 6443;
        proxy_pass k8s-apiserver;# 做反向代理到资源池
    }
}
http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    include /etc/nginx/conf.d/*.conf;
}
```

检查配置文件语法,启动nginx服务，查看已监听6443端口
```shell
[root@k8s-nginx01 ~]# nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful
[root@k8s-nginx01 ~]# systemctl start nginx 

[root@k8s-nginx01 ~]# netstat -natp | grep nginx  
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      994/nginx: master p 
```
### 搭建 keepalived 高可用服务
这里准备两台虚拟机，搭建 Nginx 的高可用集群

| IP | 操作系统 | 主机名称 |
|--|--|--|
| 192.168.2.122 | centos7.9 | k8s-nginx01|
| 192.168.2.123 | centos7.9 | k8s-nginx02|

yum 安装 keepalived 软件，在k8s-nginx01 和 k8s-nginx02 上面操作
```
yum install keepalived -y
```
修改 k8s-nginx01 和 k8s-nginx02 的keepalived配置文件，k8s-nginx01 节点作为 master
```
vi /etc/keepalived/keepalived.conf 
```
删除配置文件全部内容，添加以下内容：
```
! Configuration File for keepalived

global_defs {
   # 接收邮件地址
   notification_email {
     acassen@firewall.loc
     failover@firewall.loc
     sysadmin@firewall.loc
   }
   # 邮件发送地址
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 127.0.0.1
   smtp_connect_timeout 30
   router_id NGINX_MASTER	#lb01节点的为 NGINX_MASTER，lb02节点的为 NGINX_BACKUP
}

#添加一个周期性执行的脚本
vrrp_script check_nginx {
    script "/etc/nginx/check_nginx.sh"	#指定检查nginx存活的脚本路径
}

vrrp_instance VI_1 {
    state MASTER			#lb01节点的为 MASTER，lb02节点的为 BACKUP
    interface ens33			#指定网卡名称 ens33
    virtual_router_id 51	#指定vrid，两个节点要一致
    priority 100			#指定 k8s-nginx01 节点的为 100，k8s-nginx02 节点的为 90
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.2.100/24	#指定 VIP
    }
    track_script {
        check_nginx			#指定vrrp_script配置的脚本
    }
}
```
k8s-nginx02 节点作为 backup
```
vi /etc/keepalived/keepalived.conf 
```
删除配置文件全部内容，添加以下内容：
```
! Configuration File for keepalived

global_defs {
   # 接收邮件地址
   notification_email {
     acassen@firewall.loc
     failover@firewall.loc
     sysadmin@firewall.loc
   }
   # 邮件发送地址
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 127.0.0.1
   smtp_connect_timeout 30
   router_id NGINX_MASTER	#lb01节点的为 NGINX_MASTER，lb02节点的为 NGINX_BACKUP
}

#添加一个周期性执行的脚本
vrrp_script check_nginx {
    script "/etc/nginx/check_nginx.sh"	#指定检查nginx存活的脚本路径
}

vrrp_instance VI_1 {
    state BACKUP   #k8s-nginx01 节点的为 MASTER，k8s-nginx02节点的为 BACKUP
    interface ens33			#指定网卡名称 ens33
    virtual_router_id 51	#指定vrid，两个节点要一致
    priority 90			#指定 k8s-nginx01 节点的为 100，k8s-nginx02 节点的为 90
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.2.100/24	#指定 VIP
    }
    track_script {
        check_nginx			#指定vrrp_script配置的脚本
    }
}
```
创建 nginx 状态检查脚本：k8s-nginx01 和 k8s-nginx02 上面操作
```
vi /etc/nginx/check_nginx.sh
#!/bin/bash
#egrep -cv "grep|$$" 用于过滤掉包含grep 或者 $$ 表示的当前Shell进程ID
count=$(ps -ef | grep nginx | egrep -cv "grep|$$")

if [ "$count" -eq 0 ];then
    systemctl stop keepalived
fi
```
授权：k8s-nginx01 和 k8s-nginx02 上面操作
```
chmod +x /etc/nginx/check_nginx.sh 
```
启动 keepalived 服务：k8s-nginx01 和 k8s-nginx02 上面操作

这里一定要先启动 nginx 服务，再启动 keepalived 服务
```
systemctl start keepalived
```
查看 VIP 是否生成：k8s-nginx01 和 k8s-nginx02 上面操作
```
ip a	
```
vip 出现在 k8s-nginx01 上面，说明 keepalived 启动成功。

### 验证高可用功能
此时虚拟 ip 在 k8s-nginx01 上，在 k8s-nginx01 中使用 pkill nginx 停止 nginx 服务，再在 k8s-nginx02 上使用 ip a 命令查看地址是否进行了漂移，可以看到地址漂移到了 k8s-nginx02 节点上面。

先重启 nginx 在重启 keepalived ，使用 ip a命令查看，k8s-nginx01 节点看是否会漂移回来

可以看到漂移回 k8s-nginx01，而 k8s-nginx02 上面是没有的，说明 keepalived + nginx 高可用配置正常

访问负载均衡器测试：找到 k8s 集群中任意一个节点，使用 curl +vip的ip+端口（6433）服务

这个是 vip 服务器的地址：192.168.2.100
```
[root@k8s-master01 kubernetes]# curl https://192.168.2.100:6443
curl: (60) Peer's Certificate issuer is not recognized.
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a "bundle"
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
If you'd like to turn off curl's verification of the certificate, use
 the -k (or --insecure) option.
 ```
通过以下命令在 nginx 节点查看日志
```
tail /var/log/nginx/k8s-access.log -f
```
可以看到轮询调度把请求流量分发给两台 master，k8s-master01(192.168.2.124) 和 k8s-master02(192.168.2.121)

如果这时我们在 k8s-nginx01 中使用 pkill nginx 停止 nginx 服务，看是否会在 k8s-nginx02 中打印日志呢？

从上面可以看出 k8s-nginx02 节点和虚拟 ip 绑定，并且请求到 k8s-nginx02 中，轮询调度把请求流量分发给两台 master，k8s-master01(192.168.2.124) 和 k8s-master02(192.168.2.121)
## 配置网络插件
如果集群状态不是 Ready，是因为没有安装网络插件，执行以下命令，然后检查集群状态为 Ready 则表示部署成功
```shell
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```
## 注意事项和常见问题
### node xxx not found报错

安装到`kubeadm init`那一步执行后，出现如：node xxx not found等的报错。

```
E0519 23:21:51 .331778 25545 kubelet.go:2419] “Error getting node” err=”node \”master\” not found”
```

- 检查k8s版本是否高于v1.24.0，k8s版本 v1.24.0及以上不支持docker。
  
- 如果k8s版本符合要求，再检查`kubeadm init`传入的参数是否正确，特别是`--apiserver-advertise-address`这个参数。
  
- 可考虑检查hostname相关配置是否正确。
  
  需要确定主机名配置、hosts中(或DNS)的主机名解析是否匹配，另外还需要确认kubeadm init中IP正确，kubeadm使用配置文件初始化时，也需要核实配置文件中IP和主机名是否正确。
  
  查看当前主机名：
  
  ```shell
  hostname
  ```
  
  hostname文件的主机名：
  
  ```shell
  cat etc/hostname 
  ```
  
  etc/hosts中的主机名和解析记录：
  
  ```shell
  cat etc/hosts
  ```
  

### Docker驱动不一致

```
failed to run Kubelet: misconfiguration: kubelet cgroup driver: “systemd” is different from docker cgroup driver: “cgroupfs”"
```

这是由于docker的驱动和k8s的驱动不一致导致的，解决方式: 替换docker的cgroup为systemd。

```json
# /etc/docker/daemon.json
{
    "exec-opts": ["native.cgroupdriver=systemd"]
}
```

```shell
systemctl daemon-reload
systemctl restart docker
```

如果未能解决问题，需要再修改kubelet的Cgroup Driver

修改`/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf`文件，增加（或修改成）`--cgroup-driver=systemd` (官方推荐用systemd)

```
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=systemd"
```

修改 `/var/lib/kubelet/kubeadm-flags.env`文件，增加（或修改成）`--cgroup-driver=systemd`

```
KUBELET_KUBEADM_ARGS="--cgroup-driver=systemd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.2"
```

如果是原来正常运行的集群出现这个错误导致集群运行出现异常，修改完以上配置后，需要重启kubelet

```shell
systemctl daemon-reload
systemctl restart kubelet
```

### 执行过程健康监测状态超时

```
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory “/etc/kubernetes/manifests”. This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
```

解决方式：另开一个终端, 打开`/etc/kubernetes/manifests/etcd.yaml`，需要把`--listen-client-urls`和`--listen-peer-urls`都改成127.0.0.1:2379和127.0.0.1:2380。执行`systemctl restart kubelet`。

### 连接到8080端口失败

The connection to the server localhost:8080 was refused - did you specify the right host or port?

```shell
# 执行如下命令(非root用户):
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown ( i d − u ) : (id -u):(id−u):(id -g) $HOME/.kube/config
# 执行命令(root用户):
export KUBECONFIG=/etc/kubernetes/admin.conf
```

### resolv.conf找不到

Failed to create pod sandbox: open /run/systemd/resolve/resolv.conf: no such file or directory

将master上对应目录/run/systemd/resolve/resolv.conf文件复制一份到子节点上即可。

如还不行，可试下如下方法：
1、首先kubectl -n kube-system edit configmap coredns
2、然后注释掉loop
3、最后kubectl -n kube-system delete pod -l k8s-app=kube-dns

### no route to host

问题描述： 搭建rocketmq集群时，在机器2上 ping 机器1的ip地址可以ping通，但telnet指定端口（9876）时提示 no route to host。

#### 防火墙问题

在机器1运行`systemctl status firewalld`，发现状态是inactive，没有启动，排除防火墙问题

#### iptable配置问题

方法a： 清除所有iptables(慎用，可能影响配置）

```shell
iptables -F
```

方法b： 端口放行

```shell
# 查看已配置的规则
iptables-save

# 放开端口
iptables -I INPUT -p tcp -m tcp --dport 9876 -j ACCEPT

iptables-save
# 出现COMMIT则提交成功
```

### Docker与k8s版本匹配

不同版本的k8s需要对应不同版本的docker版本，如果想安装不同版本的k8s可以自行搜一下对应的Docker版本。

### 节点NOT READY网络未配置

Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized

Unable to update cni config: no networks found in /etc/cni/net.d

#### 解决方案1

就遇到的情况给出解决方法，基于v1.21.1版本，已安装weave，发现仍有以上错误。

发现 `/opt/cni/bin` 目录下缺少很多可执行文件，处理方式是重新安装kubernetes-cni

```bash
yum install -y kubernetes-cni
```

重新初始化问题节点即可。

#### 解决方案2

看到有的博主使用单节点k8s，不想看NOT READY状态，把 `/var/lib/kubelet/kubeadm-flags.env` 或 `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf` 下的 `--network-plugin=cni` 给去掉了。

kubelet配置 `--network-plugin=cni`，仅当CNI（容器网络插件）正确安装后才会改变状态为 READY。

上文出现的情况原因就是缺少CNI可执行文件，导致CNI初始化失败。

#### 解决方案3

将master上`/etc/cni/net.d`目录下的文件拷贝到有问题的节点上：`scp server4:/etc/cni/net.d/* /etc/cni/net.d/`

### 创建完pod后报错: Failed connect to 10.244.36.90:8000； No route to host

`deployment.yaml`的`containerPort`为`8000`，执行curl命令时提示找不到路由。执行：

```
iptables --flush
iptables -tnat --flush
```

### k8s network: stat /var/lib/calico/nodename: no such file or directory

是calico配置残留的问题，然后找到相关的calico 文件删除掉问题就解决了。

需要删除` /var/lib/calico`这个目录 和`/etc/cni/net.d/`这个目录下的calico文件就行了 。

另外还有个问题就是calico/node is not ready: BIRD is not ready: BGP not established

这个问题在换成flannel后就没有了，而且之前部署的数据库也能正常访问了。

### ImagePullBackOff 错误

docker镜像源无法访问，检查一下网络是否是通的。调试好后，输入以下命令强制刷新即可。

```shell
systemctl daemon-reload
systemctl restart docker
```

### Master当作Node使用的方法

使用下面的命令操作使得master可以作为node使用，承载pod

```shell
kubectl taint nodes --all node-role.kubernetes.io/master-
```

可能会出现下面的结果，因为taint（master标记的污点已经被去掉了）没有关系

```
taint "node-role.kubernetes.io/master" not found
taint "node-role.kubernetes.io/master" not found
```

可以用下面的

```shell
kubectl describe nodes master1 |grep Taint
```

命令确认一下taint已经没有了：```Taints: <none>```

创建应用时的`--replicas`参数改大一些，这样才会很明显的看到pod分配到master和node节点了。注：如果想只起两个副本且必须分配到两个机器上 请参考nodeSelector以及亲和性和反亲和性的的用法。

### 无法删除状态为terminating的pod

每当删除namespace或pod 等一些Kubernetes资源时，有时资源状态会卡在terminating，很长时间无法删除，甚至有时增加–force flag(强制删除)之后还是无法正常删除。这时就需要edit该资源，将字段finalizers设置为null，之后Kubernetes资源就正常删除了。

当删除pod时有时会卡住，pod状态变为terminating，无法删除pod

(1) 强制删除

`kubectl delete pod xxx -n xxx --force --grace-period=0`

(2) 如果强制删除还不行，设置finalizers为空

`kubectl patch pod xxx -n xxx -p ‘{“metadata”:{“finalizers”:null}}’`

这样pod就可以删除了。

### Windows的Docker Desktop中的k8s网络问题
```
kubectl port-forward svc/mysql 3306:3306
```

## 常用的命令

```shell
# 查看端口占用情况
netstat -anp | grep 8080

# 查看服务运行日志
journalctl -xeu kubelet

# 新建命名空间
kubectl create ns dev

# 查询全部 pod（所有命名空间）
kubectl get pods --all-namespaces

# 查询全部 pod（指定命名空间）
kubectl get pods -n

# 查询全部 node 节点
kubectl get nodes

# 查看 pod 详细信息和日志
kubectl describe pod -n
kubectl logs -f -n

# 查看 pod-yaml 文件
kubectl get pod -n -o yaml

# 通过标签查询 pod
kubectl get pod -l app= -n

# 查询 pod 具体某一条信息
kubectl -n get pods|grep |awk '{print $3}'

# 删除 pod（或通过标签 -l app=）
kubectl delete pod -n

# 删除 deployment
kubectl delete deployment -n

# 强制删除 pod
kubectl delete pod -n --force --grace-period=0

# 批量删除 pod
kubectl get pods -n ${namespace} | grep ${status} | awk '{print $1}' | xargs kubectl delete pod -n dev --force --grace-period=0
kubectl get pods -n yhdsp-dev | grep Evicted | awk '{print $1}' | xargs kubectl delete pod -n yhdsp-dev --force --grace-period=0

# 进入 pod 容器
kubectl exec -it podname -c containername -n namespace /bin/bash

# 给 node 打标签
kubectl label node <node_name> key=value  # 给节点打标签

# 查看某一个 node 标签
kubectl get node -l ""

# 查看全部 node 标签
kubectl get node --show-labels=true
```